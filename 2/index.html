<html>
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Project: Prokudin‑Gorskii Alignment</title>
<style>
:root {
--fg: #1f2937; /* slate-800 */
--muted: #6b7280; /* gray-500 */
--border: #e5e7eb; /* gray-200 */
--link: #2563eb; /* blue-600 */
}
html, body { margin: 0; padding: 0; }
body {
font: 16px/1.5 system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
color: var(--fg);
background: #fff;
}
.container { max-width: 960px; margin: 2rem auto; padding: 0 1rem; }
header { margin-bottom: 1.5rem; }
h1 { font-size: 1.8rem; margin: 0 0 0.25rem; }
header p { margin: 0; color: var(--muted); }

nav { margin: 1rem 0 2rem; }
nav a { margin-right: 1rem; color: var(--link); text-decoration: none; }
nav a:hover { text-decoration: underline; }

section { margin: 2rem 0; }
h2 { font-size: 1.4rem; margin: 0 0 0.75rem; }
h3 { font-size: 1.1rem; margin-top: 1.25rem; }
p, li { max-width: 70ch; }

/* Figures / images */
figure { margin: 0; border: 1px solid var(--border); padding: 0.5rem; border-radius: 8px; background: #fff; }
figure img { width: 100%; height: auto; display: block; }
figcaption { margin-top: 0.5rem; font-size: 0.9rem; color: var(--muted); }


.grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(220px, 1fr)); gap: 12px; }
.pair { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; }


/* Tables */
table { border-collapse: collapse; width: 100%; max-width: 100%; }
th, td { border: 1px solid var(--border); padding: 8px; text-align: left; }
th { background: #f9fafb; }
code, pre { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }


.note { color: var(--muted); font-size: 0.95rem; }
.muted { color: var(--muted); }
.small { font-size: 0.9rem; }


/* Optional narrow layout tweak */
@media (max-width: 640px) {
.pair { grid-template-columns: 1fr; }
}
</style>
</head>

<!--Start of Body-->
<body>
<header>
<div class="wrap">
<div class="title">
<h1>Image Filtering & Multi‑Resolution Blending</h1>
</div>
<nav class="toc">
<a href="#p11"><span>Part 1.1 — Convolutions from Scratch</span></a>
<a href="#p12"><span>Part 1.2 — Finite Difference Operator</span></a>
<a href="#p13"><span>Part 1.3 — DoG (Derivative of Gaussian)</span></a>
<a href="#p21"><span>Part 2.1 — Image “Sharpening”</span></a>
<a href="#p22"><span>Part 2.2 — Hybrid Images</span></a>
<a href="#p23"><span>Part 2.3 — Gaussian & Laplacian Stacks</span></a>
<a href="#p24"><span>Part 2.4 — Multiresolution Blending</span></a>
</nav>
</div>
</header>

<section id="p11">
<h2>Convolutions from Scratch</h2>
<p>
First, I implemented a convolution function using simple numpy operations. To do this, we began by padding the outside of our 
image with zeroes to allow for constant size after convolution. We then reversed the kernel and it to our image in a sliding window. We can see
the code here:
</p>
<pre><code class="language-python">def conv_two_loops(img, kernel):
    kh, kw = kernel.shape

    src = pad_zero(img, kh, kw)
    outH, outW = img.shape
    out = np.zeros((outH, outW), dtype=np.float64)
    kflip = kernel[::-1, ::-1]
    for y in range(outH):
        for x in range(outW):
            window = src[y:y+kh, x:x+kw]
            out[y, x] = np.sum(window * kflip)
    return out</code></pre>
<p>
After applying blur and difference filters to my own images, I found that the runtime of this algorithm was slower than scikit's convolve2D. 
The gradient magnitudes (from our difference kernels) was also less visible, perhaps due to the different handling of edges. We can see the blur and
gradient magnitdue results on the following image.
</p>
<div class="grid">
<figure>
<img src="image.jpg" alt="Input image for Part 1.1" />
<figcaption>Input</figcaption>
</figure>
<figure>
<img src="1.1_blur.png" alt="Blurred Image" />
</figure>
<figure>
<img src="1.1_mag.png" alt="Gradient Magnitude" />
</figure>
</div>
</section>

<section id="p12">
<h2>Part 1.2 — Finite Difference Operator </span></h2>
<p class="lead">Here, we applied the edge finding kernels to the cameraman photo. We got these edge images.</p>
<div class="grid">
<figure>
<img src="1.2_gx.png" alt="d/dx response" />
<figcaption>∂/∂x</figcaption>
</figure>
<figure>
<img src="1.2_gy.png" alt="d/dy response" />
<figcaption>∂/∂y</figcaption>
</figure>
</div>
<p class="lead">We tried different thresholds for a final gradient magnitude image, where we found the ideal to be around .3.</p>
<div class="grid">
<figure>
<img src="1.2_0.15_threshold.png" />
</figure>
<figure>
<img src="1.2_0.2_threshold.png" />
</figure>
<figure>
<img src="1.2_0.25_threshold.png" />
</figure>
<figure>
<img src="1.2_0.3_threshold.png" />
</figure>
<figure>
<img src="1.2_0.35_threshold.png" />
</figure>
</div>
</section>

<section id="p13">
<h2>Part 1.3 — Derivative of Gaussian (DoG) Filter </span></h2>
<p class="lead">Our next step was to apply a gaussian convolution to our images. To do this, we made DoG filters,
    done by convolving our 2D gaussian with the difference filters. These kernels are visualized below.
</p>
<div class="grid">
<figure>
<img src="1.3_DoGx.png" alt="DoGx" />
</figure>
<figure>
<img src="1.3_DoGy.png" alt="DoGy" />
</figure>
</div>
<p class="lead">After applying our filters to the cameraman image, we found that there was little difference between applying gaussian, then difference 
    compared to just DoG, as expected. Visualizing the edges with threshold 0.3, we found that the edges were better than using blur. They were more complete,
    and excluded some of the small artifacts that were present in the blur image.
</p>
<div class="grid">
<figure>
<img src="1.3_0.3_threshold_DoG.png" alt="DoG edges" />
</figure>
</div>
</section>

<section id="p21">
<h2>Part 2.1 — Image “Sharpening” </span></h2>
<p class="lead">The idea between sharpening is to amplify the high frequency portions of the image. To get the high frequencies,
    we subtract the gaussian blur from the original image, then we add the difference to the original image. This specifically "sharpens"
    those high frequency portions. 
</p>
<div class="grid">
<figure>
<img src="taj.jpg" alt="Sharpening input" />
<figcaption>Input</figcaption>
</figure>
<figure>
<img src="2.1_blur.png" alt="High-pass" />
</figure>
<figure>
<img src="2.1_high.png" alt="Sharpened" />
</figure>
<figure>
<img src="2.1_sharp_extra copy.png" alt="Extra Sharpened" />
</figure>
</div>
<div class="grid">
<figure>
<img src="momo.jpg" alt="Sharpening input" />
<figcaption>Input</figcaption>
</figure>
<figure>
<img src="2.1_high_dog.png" alt="High-pass" />
</figure>
<figure>
<img src="2.1_sharp_dog.png" alt="Sharpened" />
</figure>
</div>
</section>

<section id="p22">
<h2>Part 2.2 — Hybrid Images </span></h2>
<p class="lead">Combine low frequencies of one image with high frequencies of another. We made hybrid images of 
    Derek/Nutmeg, two selfies, and an ocean scene with a sunset.
</p>
<div class="grid">
<figure>
<img src="DerekPicture.jpg"  />
<figcaption>Derek</figcaption>
</figure>
<figure>
<img src="nutmeg.jpg"  />
<figcaption>Nutmeg</figcaption>
</figure>
<figure>
<img src="2.2_dereknut.jpg" alt="Hybrid image" />
<figcaption>Hybrid </figcaption>
</figure>
</div>

<div class="grid">
<figure>
<img src="sunset.jpg" />
<figcaption>Sunset</figcaption>
</figure>
<figure>
<img src="clear_ocean.jpg"  />
<figcaption>Clear Ocean</figcaption>
</figure>
<figure>
<img src="2.2_ocean_hybrid.jpg" alt="Hybrid image" />
<figcaption>Hybrid</figcaption>
</figure>
</div>

<div class="grid">
<figure>
<img src="color_image.jpeg" />
<figcaption>Photo 1</figcaption>
</figure>
<figure>
<img src="color2.jpg"  />
<figcaption>Photo 2</figcaption>
</figure>
<figure>
<img src="2.2_me_low.jpg" alt="Low Pass" />
</figure>
<figure>
<img src="2.2_me_high.jpg" alt="High Pass" />
</figure>
<figure>
<img src="2.2_me_hybrid.jpg" alt="Hybrid" />
<figcaption>Hybrid</figcaption>
</figure>
</div>
<p class="lead"> For my selfie image, I used a sigma of 3.0 for the high pass and 7.0 for the low pass. I also amplified the high
    frequencies by 7, so that they appear more strongly in the image. We also have the Fourier Transform of the final image.
</p>
<div class="grid">
<figure>
<img src="2.2_fourier.png" alt="Fourier" />
<figcaption>Fourier Transform</figcaption>
</figure>
</div>
</section>

<section id="p23">
<h2>Part 2.3 — Gaussian & Laplacian Stacks (Oraple) </span></h2>
<p class="lead">Here we implemented code for gaussian and laplacian stacks. We can visualize some of the different frequencies using 
    the built Laplacian stacks. </p>
<div class="grid">
<figure>
<img src="2.3_appleL_0.jpg" alt="L0A" />
</figure>
<figure>
<img src="2.3_appleL_2.jpg" alt="L2A" />
</figure>
<figure>
<img src="2.3_appleL_4.jpg" alt="L4A" />
</figure>
<figure>
<img src="2.3_orangeL_0.jpg" alt="L0O" />
</figure>
<figure>
<img src="2.3_orangeL_2.jpg" alt="L2O" />
</figure>
<figure>
<img src="2.3_orangeL_4.jpg" alt="L4O" />
</figure>
</div>
<p>After applying our masks, we get the following:</p>
<div class="grid">
<figure>
<img src="2.4_appleM_0.jpg" alt="M0A" />
</figure>
<figure>
<img src="2.4_appleM_2.jpg" alt="M2A" />
</figure>
<figure>
<img src="2.4_appleM_4.jpg" alt="M4A" />
</figure>
<figure>
<img src="2.4_orangeM_0.jpg" alt="M0O" />
</figure>
<figure>
<img src="2.4_orangeM_2.jpg" alt="M2O" />
</figure>
<figure>
<img src="2.4_orangeM_4.jpg" alt="M4O" />
</figure>
</div>
<p>Combining it all, we get our oraple. We used 6 levels, a sigma of 2.0 for the blurring, and 10.0 for the mask blurring.
</p>
<div class="grid">
<figure>
    <img src="2.4_oraple.jpg" alt="oraple" />
</figure>
</div>
</section>

<section id="p24">
<h2>Part 2.4 — Multiresolution Blending </span></h2>
<p class="lead">We have additional blended images, of a monet painting with the real life location, and the ocea images from before.</p>
<div class="grid">
<figure>
<img src="monet_lilies.jpg" />
<figcaption>Image 1</figcaption>
</figure>
<figure>
<img src="rl_lilies.jpg" />
<figcaption>Image 2</figcaption>
</figure>
<figure>
<img src="2.4_monet.jpg" alt="Monet" />
<figcaption>Final Blend (Monet)</figcaption>
</figure>
</div>
<div class="grid">
<figure>
<img src="sunset.jpg" />
<figcaption>Image 1</figcaption>
</figure>
<figure>
<img src="clear_ocean.jpg" />
<figcaption>Image 2</figcaption>
</figure>
<figure>
<img src="2.4_ocean2.jpg" alt="Ocean" />
<figcaption>Final Blend (Sunset)</figcaption>
</figure>
</div>
</section>

</body>
</html>